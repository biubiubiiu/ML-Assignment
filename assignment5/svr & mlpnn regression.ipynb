{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db8f41b9",
   "metadata": {},
   "source": [
    "# SVR & MLPNN Regression Exercise\n",
    "\n",
    "The target of this assignment is to design a regression system to predict [boston housing prices](https://www.kaggle.com/vikrishnan/boston-house-prices). The regression algorithms should contain support vector regression and MLPNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f16d2",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1-Packages](#1)\n",
    "- [2-Load the Dataset](#2)\n",
    "- [3-Support Vector Regression](#3)\n",
    "- [4-MLPNN Regression](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfa2ab",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "\n",
    "## 1 - Packages\n",
    "\n",
    "First import all the packages needed during this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7f0eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e340e",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "\n",
    "## 2 - Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35e5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "y_train = np.array(y_train).reshape(-1,)\n",
    "y_test = np.array(y_test).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3777421b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (404, 13)\n",
      "Training labels shape:  (404,)\n",
      "Test data shape:  (102, 13)\n",
      "Test labels shape:  (102,)\n"
     ]
    }
   ],
   "source": [
    "# As a sanity check, print out the size of the training and test data\n",
    "print('Training data shape: ', x_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa0ec2",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "## 3 - Support Vector Regression\n",
    "\n",
    "The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only minor differences. The main idea behind SVR is to decide a decision boundary distance from the original hyperplane such that data points closest to the hyperplane or the support vectors are within that boundary line.\n",
    "\n",
    "Optimization objective:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min \\quad & \\frac{1}{2}||w||^2+C\\sum_{i=1}^{N}(\\xi_{i}+\\xi_{i}^{*}) \\\\\n",
    "\\text { subject to } \\quad & y_i-wx_i-b \\le \\varepsilon+\\xi_{i} \\\\\n",
    "& wx_i+b-y_i \\le \\varepsilon+\\xi_{i}^{*} \\\\\n",
    "& \\xi_{i},\\xi_{i}^{*} \\ge 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Mapping function:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K(x_i, x_j)=\\exp(-\\gamma||x_i-x_j||^2) \\\\\n",
    "y=\\sum_{i=1}^N(\\alpha_{i}-\\alpha_{i}^{*}) \\cdot K(x_i, x)+b \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this section, we use *Radial basis function (rbf)* as kernel function to build our SVM. The tolerence factor $C$ is set to $10^{2}$ and the $\\gamma$ in rbf is set to $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c7e288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100.0, gamma=0.1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "model = SVR(kernel='rbf', C=1e2, gamma=0.1)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06800310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE： 6.996249978087986\n",
      "MSE： 101.36142683921726\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "pred = model.predict(x_test)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "print('MAE：', mae)\n",
    "print('MSE：', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd9927",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "\n",
    "## 4 - MLPNN Regression\n",
    "\n",
    "In this section, we would build a three-layer MLPNN using `sklearn`. *ReLU* is selected as activation funciton for each hidden layer and Adam optimizer is applied in the training stage.\n",
    "\n",
    "The mapping function of the network can be describe as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "O_{1}&=ReLU(w_{1}^{T}X+b_{1}) \\\\\n",
    "O_{2}&=ReLU(w_{2}^{T}X+b_{2}) \\\\\n",
    "O_{3}&=ReLU(w_{3}^{T}X+b_{3}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $O_{1}, O_{2}, O_{3}$ is the output of layer 1, layer 2 and layer 3 respectively.\n",
    "\n",
    "We use *Mean Squared Error (MSE)* as loss function, which is defined as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N}\\sum_{i=1}^{N}(f(x)-y)^2\n",
    "$$\n",
    "\n",
    "where $f(x)$ is the output of MLPNN and $y$ is the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c88581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(50, 25))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(50, 25),\n",
    "    activation='relu',\n",
    "    solver='adam'\n",
    ")\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc60491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE： 6.137284501473741\n",
      "MSE： 89.44981470868586\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "pred = model.predict(x_test)\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "print('MAE：', mae)\n",
    "print('MSE：', mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
